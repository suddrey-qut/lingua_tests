#!/usr/bin/env python
import rospy
import ros_numpy
import cv2
import json
import numpy as np
import message_filters
import tf2_ros

from lingua_world.msg import WorldState
from sensor_msgs.msg import Image, CameraInfo
from sensor_msgs.msg import JointState
from geometry_msgs.msg import TransformStamped

from threading import Lock

from rv_msgs.srv import ProcessObservation, ProcessObservationRequest
from rv_msgs.msg import Observation, Detection
from rv_msgs.msg import ManipulatorState
from rv_msgs.msg import MoveToPoseActionGoal, MoveToPoseActionResult

global out_mask
br = None

def block_cb(msg):
  global blocked
  blocked = True

def unblock_cb(msg):
  global blocked
  blocked = False

def image_cb(depth_info, depth_image, rgb_info, rgb_image):
  print('frame')
  state = rospy.wait_for_message('/arm/state', ManipulatorState)
  joint_states = rospy.wait_for_message('/joint_states', JointState)

  if blocked:
    print('Blocked')
    return

  if sum(joint_states.position[-2:]) < 0.074:
    print('fingers closed')
    return

  global out_mask
  input_image = ros_numpy.numpify(rgb_image)
  # cv2.imshow('input', input_image)
  img = input_image.astype(np.float32) / 255.
  
  r = img[:,:,0] # 51
  g = img[:,:,1]
  b = img[:,:,2] # 45

  omega = r+g+b

  channels = []
  channels.append(('red', r, .5))
  channels.append(('blue', b, .5))
  channels.append(('green', g, .45))

  display_img = np.array([])
  i = 0

  observation = Observation(
    depth_info=depth_info,
    depth_image=depth_image,
    rgb_info=rgb_info,
    rgb_image=rgb_image
  )

  world_state = WorldState()
  
  for color_name, channel, threshold in channels:
    X = channel / omega

    _, mask = cv2.threshold(X, threshold, 255, cv2.THRESH_BINARY)
    mask = mask.astype(np.uint8)

    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    out_mask = np.ones(shape=input_image.shape, dtype=np.uint8)
    out_mask[:,:,0] = mask
    out_mask[:,:,1] = mask
    out_mask[:,:,2] = mask
    
    observation.detections = []
    for contour in contours:
      area = cv2.contourArea(contour)

      if area < 1000:
        continue

      print('Spotted:', color_name)


      detection = Detection()

      x,y,width,height = cv2.boundingRect(contour)

      x = x + width / 2 - 5
      y = y + height / 2 - 5

      width = 10
      height = 10
      
      detection.y_top = y
      detection.x_left = x
      
      detection.height = height
      detection.width = width
      
      detection.cropped_rgb=ros_numpy.msgify(Image, ros_numpy.numpify(observation.rgb_image)[y:y+height,x:x+width,:], 'rgb8')
      detection.cropped_depth=ros_numpy.msgify(Image, ros_numpy.numpify(observation.depth_image)[y:y+height,x:x+width], '32FC1')
      detection.cropped_mask=ros_numpy.msgify(Image, mask[y:y+height,x:x+width], 'mono8')

      cv2.rectangle(out_mask, (x,y), (x+width,y+height), (255,0,0), 2)
      
      detection.class_label = 'block'
      observation.detections.append(detection)
    
    print(color_name, len(observation.detections))
    if len(observation.detections) == 0:
      continue

    result = ggcnn(observation)
    
    for idx, detection in enumerate(result.result.detections):
      print('Detection', color_name)

      if detection.grasp_pose.pose.position.z < 0.04:
        continue
      print(color_name)

      t = TransformStamped()

      t.header.stamp = rospy.Time.now()
      t.header.frame_id = "panda_link0"
      t.child_frame_id = '{}_{}'.format(color_name, idx)
      t.transform.translation.x = detection.grasp_pose.pose.position.x
      t.transform.translation.y = detection.grasp_pose.pose.position.y
      t.transform.translation.z = detection.grasp_pose.pose.position.z
      
      t.transform.rotation.x = detection.grasp_pose.pose.orientation.x
      t.transform.rotation.y = detection.grasp_pose.pose.orientation.y
      t.transform.rotation.z = detection.grasp_pose.pose.orientation.z
      t.transform.rotation.w = detection.grasp_pose.pose.orientation.w

      br.sendTransform(t)
      
      world_state.objects.append(json.dumps({
          "attributes" : [ 
              {
                  "value" : [ 
                      "block"
                  ],
                  "key" : "class_label"
              },
                            {
                  "value" : [ 
                      color_name
                  ],
                  "key" : "color"
              }
          ],
          "transient" : True,
          "position" : {
              "header" : {
                  "stamp" : 0,
                  "frame_id" : "panda_link0"
              },
              "pose" : {
                  "position" : {
                      "x" : detection.grasp_pose.pose.position.x,
                      "y" : detection.grasp_pose.pose.position.y,
                      "z" : detection.grasp_pose.pose.position.z
                  },
                  "orientation" : {
                      "x" : detection.grasp_pose.pose.orientation.x,
                      "y" : detection.grasp_pose.pose.orientation.y,
                      "z" : detection.grasp_pose.pose.orientation.z,
                      "w" : detection.grasp_pose.pose.orientation.w
                  }
              }
          }
      }))

      # object_evidence.properties.append(Property(attribute='orientation', pdf=PDF(
      #   type=PDF.GAUSSIAN,
      #   dimensions=4,
      #   data=[PDF.GAUSSIAN, qx, qy, qz, qw, covariance, 0, 0, covariance, 0, covariance]
      # )))

    # if color_name == 'green':
    # cv2.imshow(color_name, out_mask)
    # cv2.waitKey(30)
      
  publisher.publish(world_state)
  rospy.sleep(1)
    
blocked = False

if __name__ == '__main__':
  rospy.init_node('block_detector')

  br = tf2_ros.TransformBroadcaster()
  publisher = rospy.Publisher('/world_state', WorldState, queue_size=100)

  sub1 = rospy.Subscriber('/arm/cartesian/pose/goal', MoveToPoseActionGoal, block_cb, queue_size=10)
  sub2 = rospy.Subscriber('/arm/cartesian/pose/result', MoveToPoseActionResult, unblock_cb, queue_size=10)

  topic_names = [
    '/camera/aligned_depth_to_color/camera_info',
    '/camera/depth/image_meters_aligned',
    '/camera/color/camera_info',
    '/camera/color/image_rect_color'
  ]
  topic_classes= [
    CameraInfo, Image, CameraInfo, Image
  ]

  ggcnn = rospy.ServiceProxy('/service/ggcnn', ProcessObservation, persistent=True)
  ggcnn.wait_for_service()

  _subscribers = [ message_filters.Subscriber(topic_name, topic_classes[idx], queue_size=1, buff_size=99999999) 
            for idx, topic_name in enumerate(topic_names) ]
  f = message_filters.ApproximateTimeSynchronizer(_subscribers, 1, 0.4, allow_headerless=True)
  f.registerCallback(image_cb)

  
  rospy.spin()
  